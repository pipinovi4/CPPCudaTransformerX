{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f34397-157a-4e0c-8089-642592f1b985",
   "metadata": {},
   "source": [
    "# Multi-Head Attention Model Training and Testing on Synthetic Data\n",
    "\n",
    "This notebook demonstrates the training and testing of a multi-head attention model using PyTorch on synthetic data. We will define the model, prepare the data, and then go through the training and testing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2561b273-dd94-4c90-a173-1cf4be82c976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display: flex; align-items: center; justify-content: center; border: 2px solid black; padding: 20px; margin: 20px; background-color: #f9f9f9; height: 50%px; width: 30%\">\n",
       "        <div style=\"text-align: center;\">\n",
       "            <h2 style=\"margin-bottom: 10px;\">Last run notebook:</h2>\n",
       "            <span style=\"font-size:18px; font-weight:bold;\">2024-08-15 12:57:27</span>\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "from datetime import datetime\n",
    "\n",
    "def display_last_run_notebook():\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    html_content = f\"\"\"\n",
    "    <div style=\"display: flex; align-items: center; justify-content: center; border: 2px solid black; padding: 20px; margin: 20px; background-color: #f9f9f9; height: 50%px; width: 30%\">\n",
    "        <div style=\"text-align: center;\">\n",
    "            <h2 style=\"margin-bottom: 10px;\">Last run notebook:</h2>\n",
    "            <span style=\"font-size:18px; font-weight:bold;\">{current_time}</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html_content))\n",
    "\n",
    "display_last_run_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a95fa98d-fef5-46a7-9363-f8df410fdd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"display: flex; border: 2px solid black; padding: 10px; margin: 10p; background-color: #f9f9f9;\">\n",
       "        <div style=\"margin-right: 20px; text-align: center;\">\n",
       "            <h2 style=\"margin-bottom: 10px;\">Epoch 1/5 Loss:</h2>\n",
       "            <span style=\"font-size:18px; font-weight:bold;\">2.0825982456207277</span>\n",
       "        </div>\n",
       "        <div style=\"margin-right: 20px; text-align: center;\">\n",
       "            <h2 style=\"margin-bottom: 10px;\">Epoch 2/5 Loss:</h2>\n",
       "            <span style=\"font-size:18px; font-weight:bold;\">2.0799415798187257</span>\n",
       "        </div>\n",
       "        <div style=\"margin-right: 20px; text-align: center;\">\n",
       "            <h2 style=\"margin-bottom: 10px;\">Epoch 3/5 Loss:</h2>\n",
       "            <span style=\"font-size:18px; font-weight:bold;\">2.0799079780578613</span>\n",
       "        </div>\n",
       "        <div style=\"margin-right: 20px; text-align: center;\">\n",
       "            <h2 style=\"margin-bottom: 10px;\">Epoch 4/5 Loss:</h2>\n",
       "            <span style=\"font-size:18px; font-weight:bold;\">2.079256364822388</span>\n",
       "        </div>\n",
       "        <div style=\"margin-right: 20px; text-align: center;\">\n",
       "            <h2 style=\"margin-bottom: 10px;\">Epoch 5/5 Loss:</h2>\n",
       "            <span style=\"font-size:18px; font-weight:bold;\">2.0788687171936036</span>\n",
       "        </div>\n",
       "        <div style=\"margin-right: 20px; text-align: center;\">\n",
       "            <h2 style=\"margin-bottom: 10px;\">Test Loss:</h2>\n",
       "            <span style=\"font-size:18px; font-weight:bold;\">2.078879325866699</span>\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_experiment_summary():\n",
    "    html_content = f\"\"\"\n",
    "    <div style=\"display: flex; border: 2px solid black; padding: 10px; margin: 10p; background-color: #f9f9f9;\">\n",
    "        <div style=\"margin-right: 20px; text-align: center;\">\n",
    "            <h2 style=\"margin-bottom: 10px;\">Epoch 1/5 Loss:</h2>\n",
    "            <span style=\"font-size:18px; font-weight:bold;\">2.0825982456207277</span>\n",
    "        </div>\n",
    "        <div style=\"margin-right: 20px; text-align: center;\">\n",
    "            <h2 style=\"margin-bottom: 10px;\">Epoch 2/5 Loss:</h2>\n",
    "            <span style=\"font-size:18px; font-weight:bold;\">2.0799415798187257</span>\n",
    "        </div>\n",
    "        <div style=\"margin-right: 20px; text-align: center;\">\n",
    "            <h2 style=\"margin-bottom: 10px;\">Epoch 3/5 Loss:</h2>\n",
    "            <span style=\"font-size:18px; font-weight:bold;\">2.0799079780578613</span>\n",
    "        </div>\n",
    "        <div style=\"margin-right: 20px; text-align: center;\">\n",
    "            <h2 style=\"margin-bottom: 10px;\">Epoch 4/5 Loss:</h2>\n",
    "            <span style=\"font-size:18px; font-weight:bold;\">2.079256364822388</span>\n",
    "        </div>\n",
    "        <div style=\"margin-right: 20px; text-align: center;\">\n",
    "            <h2 style=\"margin-bottom: 10px;\">Epoch 5/5 Loss:</h2>\n",
    "            <span style=\"font-size:18px; font-weight:bold;\">2.0788687171936036</span>\n",
    "        </div>\n",
    "        <div style=\"margin-right: 20px; text-align: center;\">\n",
    "            <h2 style=\"margin-bottom: 10px;\">Test Loss:</h2>\n",
    "            <span style=\"font-size:18px; font-weight:bold;\">2.078879325866699</span>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html_content))\n",
    "\n",
    "# Call the function to display the summary\n",
    "display_experiment_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68b3bb7-e91e-4ea2-9a4f-2f4f15711ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Constants for the experiment\n",
    "# MAX_SEQUENCE_LENGTH: Maximum length of input sequences\n",
    "# HIDDEN_DIM: Hidden dimension size in the model\n",
    "# NUM_HEADS: Number of attention heads in the multi-head attention layer\n",
    "# HEAD_DIM: Dimension of each attention head\n",
    "# LEARNING_RATE: Initial learning rate for the optimizer\n",
    "# DECAY_RATE: Factor by which the learning rate is reduced at each step\n",
    "# NUM_EPOCHS: Number of epochs to train the model\n",
    "# BATCH_SIZE: Number of samples per batch\n",
    "MAX_SEQUENCE_LENGTH = 32\n",
    "HIDDEN_DIM = 64\n",
    "NUM_HEADS = 8\n",
    "HEAD_DIM = HIDDEN_DIM // NUM_HEADS\n",
    "LEARNING_RATE = 0.001\n",
    "DECAY_RATE = 0.6\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e70f47-9771-49a6-b129-a315b2848ed4",
   "metadata": {},
   "source": [
    "## Define the Multi-Head Attention Model\n",
    "\n",
    "We define a `MultiHeadAttentionModel` class that includes an embedding layer, a multi-head attention mechanism, and a fully connected layer. This model will take sequences as input and output a transformed sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02917128-842e-44c9-94d9-136117a2a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Multi-Head Attention model using PyTorch\n",
    "class MultiHeadAttentionModel(nn.Module):\n",
    "    def __init__(self, max_seq_len, num_heads, head_dim):\n",
    "        super(MultiHeadAttentionModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(max_seq_len, head_dim * num_heads)\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=head_dim * num_heads, num_heads=num_heads)\n",
    "        self.fc = nn.Linear(head_dim * num_heads, head_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.multihead_attention(x, x, x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = MultiHeadAttentionModel(MAX_SEQUENCE_LENGTH, NUM_HEADS, HEAD_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c37706d-c0c0-4ebe-bfcd-42a0b93b33dc",
   "metadata": {},
   "source": [
    "## Optimizer and Learning Rate Scheduler\n",
    "\n",
    "Next, we define the optimizer and a learning rate scheduler. The Adam optimizer is chosen for its effectiveness in training deep learning models. The learning rate will decay by a factor of `DECAY_RATE` after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9474c549-0863-4ec3-8709-2de7f3fd37eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer with a learning rate scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=DECAY_RATE)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f5463-89f6-40b0-b054-040ea5636133",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "For this experiment, we generate synthetic input and target data. The data is loaded into a `DataLoader`, which handles batching and shuffling during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48ff0307-9167-4724-8726-bd022f0359c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the input data generation to create 3D tensors\n",
    "input_data = torch.randint(0, MAX_SEQUENCE_LENGTH, (1000, MAX_SEQUENCE_LENGTH))\n",
    "target_data = torch.randint(0, HEAD_DIM, (1000, MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(input_data, target_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23948832-d4dc-468d-b257-a3d71bebcdbd",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "We train the model over a specified number of epochs. In each epoch, the model performs forward and backward passes on the training data, and the optimizer updates the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed5f2d80-b8d2-4245-a964-7f517fe5dbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.0825982456207277\n",
      "Epoch [2/5], Loss: 2.0799415798187257\n",
      "Epoch [3/5], Loss: 2.0799079780578613\n",
      "Epoch [4/5], Loss: 2.079256364822388\n",
      "Epoch [5/5], Loss: 2.0788687171936036\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, HEAD_DIM), targets.view(-1))\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: {epoch_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab4ff7e-7b30-4f41-ab44-93d1b1f8eb16",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "\n",
    "After training, we evaluate the model on the same data to observe the test loss. This helps us understand how well the model has learned the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1082547-6434-442b-a2a9-995782fa937e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.078879325866699\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, HEAD_DIM), targets.view(-1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "print(f'Test Loss: {test_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d75c905-8973-4f72-bf36-e5119c14b934",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the training and testing of a simple multi-head attention model on synthetic data using PyTorch. The results show the model's ability to learn from and generalize to the data provided."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
