{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac583c68-7c29-473c-80f5-038327b304db",
   "metadata": {},
   "source": [
    "# **1. Introduction:**\n",
    "\n",
    "The goal is to demonstrate the training of a BaselineEmbedding layer within a simple neural network model. The model will be trained on a synthetic dataset to classify sequences of token indices into binary labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929c067e-0a87-4823-8768-94049fc0db8e",
   "metadata": {},
   "source": [
    "# **2. Methodology:**\n",
    "\n",
    "## **2.1. Embedding Layer Architecture:** \n",
    "\n",
    "We create a synthetic dataset of sequences of token indices. Each sequence will be randomly generated, and the corresponding label will be either 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3d98ab-c22e-482b-936b-2b6738757e4c",
   "metadata": {},
   "source": [
    "## **2.2. Model Architecture:**\n",
    "The model consists of the following components:\n",
    "\n",
    "BaselineEmbedding: Converts token indices into embeddings.\n",
    "A simple feedforward neural network with one hidden layer, followed by a linear layer to produce the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e378a6c-7776-402c-ad5a-20117b4ca163",
   "metadata": {},
   "source": [
    "## **2.3. Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e96ebd-5b08-4443-bd83-c1f51aa56512",
   "metadata": {},
   "source": [
    "### BaseLineEmbedding model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2f7cd59-b6cf-496b-bb1b-5f7e7346cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class BaselineEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(BaselineEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim * 10, 64)  # Assuming sequence length of 10\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the embeddings\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bfd27a-ba78-4c95-9e2c-5e18d975c584",
   "metadata": {},
   "source": [
    "### Generation dummy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13b815bd-2b5b-4e34-bec6-782ba236ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic dataset\n",
    "def generate_synthetic_data(num_samples, vocab_size, sequence_length):\n",
    "    X = torch.randint(0, vocab_size, (num_samples, sequence_length))\n",
    "    y = torch.randint(0, 2, (num_samples, 1)).float()  # Binary labels\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6edca4c-0489-49f5-b7a3-364f36119b83",
   "metadata": {},
   "source": [
    "### **Example Usage:**\n",
    "\n",
    "An example usage of the BaselineEmbedding class is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6e34afe-d3fb-4d42-bf71-11e01807d9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.6568\n",
      "Epoch [2/20], Loss: 0.7804\n",
      "Epoch [3/20], Loss: 0.5869\n",
      "Epoch [4/20], Loss: 0.5828\n",
      "Epoch [5/20], Loss: 0.4992\n",
      "Epoch [6/20], Loss: 0.4804\n",
      "Epoch [7/20], Loss: 0.6007\n",
      "Epoch [8/20], Loss: 0.3533\n",
      "Epoch [9/20], Loss: 0.6057\n",
      "Epoch [10/20], Loss: 0.5197\n",
      "Epoch [11/20], Loss: 0.3698\n",
      "Epoch [12/20], Loss: 0.2110\n",
      "Epoch [13/20], Loss: 0.2667\n",
      "Epoch [14/20], Loss: 0.1055\n",
      "Epoch [15/20], Loss: 0.1590\n",
      "Epoch [16/20], Loss: 0.1575\n",
      "Epoch [17/20], Loss: 0.1836\n",
      "Epoch [18/20], Loss: 0.0807\n",
      "Epoch [19/20], Loss: 0.1019\n",
      "Epoch [20/20], Loss: 0.0827\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    vocab_size = 50  # Vocabulary size\n",
    "    embedding_dim = 16  # Embedding dimension\n",
    "    sequence_length = 10  # Length of each sequence\n",
    "    num_samples = 1000  # Number of samples in the dataset\n",
    "    num_epochs = 20  # Number of training epochs\n",
    "    batch_size = 32  # Batch size for training\n",
    "    learning_rate = 0.001  # Learning rate\n",
    "\n",
    "    # Generate synthetic data\n",
    "    X, y = generate_synthetic_data(num_samples, vocab_size, sequence_length)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataset = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Model, loss function, and optimizer\n",
    "    model = BaselineEmbedding(vocab_size, embedding_dim)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_X, batch_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041af2cf-d827-48e9-82c7-70f5f569c813",
   "metadata": {},
   "source": [
    "**3. Results:**\n",
    "\n",
    "The training loop iterates over the synthetic dataset for a specified number of epochs. The loss value is printed after each epoch to monitor the training process. The model learns to classify the sequences, and the loss value decreases as training progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e95fd-aa73-4cda-b972-38d8e7a875d2",
   "metadata": {},
   "source": [
    "**4. Discussion:**\n",
    "   \n",
    "This example demonstrates the integration of the BaselineEmbedding layer into a simple neural network model. The synthetic dataset serves as a straightforward example, but the same architecture could be applied to real-world datasets. The model could be expanded with additional layers, more complex architectures, or larger datasets for more advanced tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da409b02-2265-4a91-83b2-b2d09cb51ee8",
   "metadata": {},
   "source": [
    "**5. Conclusion:**\n",
    "   \n",
    "The `BaselineEmbedding` class was successfully trained as part of a simple neural network model. This demonstrates the practical application of embedding layers in NLP tasks and lays the groundwork for more complex models in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
